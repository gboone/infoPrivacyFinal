<!-- 
A proposal for an Open Policymaking Interface, a new model for privacy policy.
-->
# Protect my Data

## Introduction

<!-- Goog settlement with FTC -->
Search and Internet services giant Google recently settled a privacy dispute with the Federal Trade Commission (FTC) with a penalty of $7 million and an instruction to beef up their privacy policies related to Google's street view service. At issue in that case was not any invasion of privacy carried out by the company's photo-snapping cars, but the actions of a few of their drivers who took advantage of security flaws in household wireless networks to steal passwords, emails, and troves of other personal information from private computers. At the same time Google's biggest competitors have taken aim at the company's handling of personal data through their email, search and other Internet tools. Microsoft's *"Don't get Scroogled"* marketing campaign highlights the 'reading of email' Google does to serve ads to email, and other data sharing pracitces the Big G engages in. <!-- The big question in scenarios like these is, 'why surrender personal data if you don't have to?' -->

<!-- SCOTUS -->
From another angle, the Supreme Court of the United States considered a case this year that asked the extent to which there was a legitimate interest in sharing of medical information between doctors and pharmaceutical companies. Should the pharmaceutical companies be allowed to turn those data over to a third party processor for marketing purposes, or do the companies have a First Amendment right to do as they please with the data? The third-party doctrine adopted first in telecommunications privacy policy offers some guidance: that once data is surrendered, the third party may do as he pleases with it. A countervailing interest lies with our understanding of health care privacy under HIPAA, information exchanged between a doctor and patient should not breech that dyad except under special circumstances, and marketing does not seem to be among them. 

Among these are other controversies that have involved government use of data. [The IRS in April claimed it could read citizen emails without a search warrant](http://thehill.com/blogs/hillicon-valley/technology/292989-irs-claims-it-can-read-emails-without-a-warrant). Traditionally government agencies would need a warrant to intercept mail between citizens, but insufficiencies in the Electronic Communications Privacy Act leaves some grey area over when and by what means (warrant or subpoena) the government may access email held by third parties (like Google and Microsoft). Unmanned Aerial Vehicles, also known as drones, present a similar problem for citizen privacy. The technology has potential 

<!-- Thesis statement: a privacy policy based on open gov, focused on education and encouragement -->
These two situations are a good transom into some of the more puzzling privacy issues policymakers face with information privacy, these, added atop some other classic examples (people losing jobs to Facebook pictures in the wrong hands, people being charged with crimes based on evidence retrieved from digital archives, and so on), further exacerbate the anxiety policy makers should experience when thinking about the legislative lift required to 'solve the problem.' But what if the problem is simpler than it seems, so simple that an act of Congress may be over-solving the problem. This paper will argue for a privacy policy for the United States based on principles of open government, borrowing lightly from the world of computer programming, and focused on encouraging good behavior among private entities and education among citizens.

## The problem: not breaking anything

<!-- Data are everywhere, using them can be profoundly useful. -->
Data surround us. Researchers Bounie and Gile estimated the volume of data produced globally in 2008 was nearly 15 exabytes, or 16 billion gigabytes or two billion full DVDs. Some of these data are being collected by private companies who often store them and send them to other third-party processors or sell them to advertisers to build web services. This exchange of personal data is not only what keeps these services alive, but also what makes them work so well. Google's Gmail service, for example, uses data about who you email and how frequently to try and guess who you mean when you start typing just a few letters. It also recommends other people who you frequently email in conjunction with others to include on a message. When I email co-worker X, it recommends co-worker Y and friend Z, because I often email X, Y, and Z together. 

<!-- Data can make existing tech more efficient -->
These kind of predictive services are amazingly efficient. Before Google rolled out Instant search in 2010 the typical search took about 9 seconds to type. Since then they estimated "more than 3.5 billion seconds" will be saved each day simply by reducing that nine seconds down to between two and five ([Google, 2010.](http://www.google.com/insidesearch/features/instant/about.html)). This product built off of Google's existing search algorithms, but also the data collected about what people _mean_ when they search. Without the collection of troves of data about what word it's users most frequently completed when they type a given combination of letters, they could not have built such a well functioning tool. 

<!-- What happens to data when they leave Google's capable hands? -->
The drawback, though, is what Google do with those data outside of building that well-functioning tool. They also share some of those data with advertising companies, some of which they own, to generate online ads for their users. Google's business model is built on this kind of sharing. What those advertisers do with Google users' data, who they sell it to, and what those people do with it are completely hidden from view. When a law enforcement agency goes to Google or one of the partners they have sold data to, what happens to those data? Do those agencies need warrants to access it? Additionally is the question of what is done with these data when they are stolen or co-opted by nefarious entities and autocratic governments. This concern is especially pervasive in European countries like the Netherlands where such a government using databases to round up individuals is a real and recent memory.

<!-- Compare Google Search with non-data-using competitor. --> 
Google search competitor DuckDuckGo offers a search tool that does not collect person data, and is in some ways an inferior product because of it. A search for "Boston Marathon" on Tuesday, April 16, 2013, a day after the notorious bombing of the world's oldest annual city marathon, on DuckDuckGo showed only generic results about the race, nothing about the tragedy. Google, comparatively, was able to deliver three news results at the top of the results, and an additional eight results in the normal search results. Despite displaying more results and not keeping a copy of any search data, DuckDuckGo's results were inferior. Similarly, searches for Cyber Intelligence Sharing and Protection Act (CISPA) and the Electronic Communications Protection Act (ECPA) returned generic results and old news articles on DuckDuckGo while Google led off with current news as well as generic information about these active bills and debates before the American Congress.

<!-- The tension is between data collection and making things work -->
Herein lies the tension between privacy and making things that work. Most of Google's products would not work as well without the data they sell to their advertisers and put to work in their products. They certainly would not have the capital to make new products without selling something, and click-through data are easier to sell than other models like subscriptions and premium services. On the one hand, an individual can search DuckDuckGo quietly and need not worry about third parties connecting search terms back to him or her. On the other they are getting a clearly inferior product. Searches for _Boston Marathon_, _Boston Marathon 2013_ both returned nothing related to the bombing, while a search for _Boston Marathon bombing_ did, few of the results were from major media outlets whereas Google's results floated links to the New York Times, CNN, and Fox above blogs and smaller and less reputable sources. In fact, after the 14th result, none of DuckDuckGo's results even mention the bombing. While not all of the discrepancy can be linked directly to Google's use of user data to inform searches, the differences are telling and make Google's products invaluable resources.

<!-- Not just search -->
Other technologies face similar issues. Unmanned aerial vehicles, so-called 'drones,' have the potential to give law enforcement as well as private individuals the ability to use cameras and other sensors mounted on an aircraft to gather data about the world around them and even fire missiles at potential targets. At the same time they could be used by the digital peeping Tom who seeks to spy on his neighbor, or by a national security program to track individuals without their consent. This list goes on and on, from GPS trackers in cellphones and automobiles, to Internet services powering the devices that make up the 'Internet of things,' data collection, storage, aggregation and processing are linchpins of any new technology's effectiveness and potentially its most dangerous component.

<!-- Policymakers should pay special attention to this balance -->
Any public policy made on information privacy must be done with special attention given to this delicate balance between building good technology, and violating legitimate privacy interests. First party processing, that is, should be a high priority for lawmakers seeking to solve the information privacy problem. While solutions like Mozilla Firefox's default third-party cookie blocking and the Do Not Track standard built into other browsers are compelling, they only solve part of the problem. These solutions give users more control over what is shared, but leave them still without much guidance as to how to discover what happens to the data they do share, and what they can do about it.

<!-- Past regulation attempts -->
Past regulation attempts have failed to respond effectively to the problems thrown by these new technologies. This is in part because they are outdated. ECPA was enacted some 27 years ago, and the only law passed since pertaining to electronic data transmissions applies only to children (the Children's Online Privacy Protection Act, [COPPA]). There is a sense of urgency to correct this: the longer Congress waits to act, the further technology will progress, the larger the databases will grow, and the more out of hand the issues around data use will become. The European Union sought to update its 1995 directive on electronic privacy in 2013 using the one policy to rule them all approach they employed in the past, but the United States has yet to enact any new legislation, mostly leaving any online privacy action to agencies like the Federal Trade Commission (FTC) to apply what laws are already on the books to the online world. US agencies have crafted a plurality of privacy recommendations and policies mostly derived from some understanding of the Fair Information Practices Principles (FIPPs), a practice that lends some credibility to Helen Nissenbaum's ideas about privacy being context specific.

<!-- Context: different problems, different policies -->
Indeed these various problems each seem like discrete areas where privacy should be protected distinctly from the others. Nissenbaum argued the social norms attached to different kinds of information flows should inform privacy policy. There is economic incentive for society to individuals being forced into the credit system (fewer bad loans being issued) and to the individuals themselves (potentially better rates and access to credit when it is needed) and so we have a policy of no-opt out in the US. In this case our right to privacy is consistent with the expectations we have of other parties to handle information about our creditworthiness. Nissenbaum uses the case of health care. We expect information shared with our doctors in clinics stays between us and the other medical professionals our doctors need to consult to return a diagnosis and care plan. Anything outside of that would be a breach of our medical privacy we could regulate.

<!-- Something is compelling about context -->
Nissenbaum's ideas are compelling, to be sure, and she's correct in noting that the current system of notice and choice is not working. It only results, as Cate argued, in long privacy policies nobody is reading. But the problem is not that these data are or could be used outside of a given context of social norms, because indeed they will be, it is that new technologies, once deployed into a society, create situations for which there are no new social norms. <!-- And as Langdon Winner has elegantly argued, once a technology's black box is open, there is no turning back. -->

<!-- Something remains compelling about FIPPS -->
For Nissenbaum the solution to privacy policy problems is a surgical, sociological approach to understanding the situation and making policy around the appropriate flow of data for that context. Fred Cate takes a more legalistic approach. Cate dissected the history of the "fair information practice principles," (FIPPs) to ultimately ask "which FIPPs should we follow?" What Cate found was that the interesting and useful idea of a universal declaration of principles to guide all data flows has failed spectacularly. What we have accumulated since the 1970s is a collection of enumerated lists all of which are different versions of the same idea, all of which result in long, complicated policies nobody is really sure how to enforce.

## A Way Forward: Government as a Platform

<!-- We need something, just not any of that stuff -->
The perplexing problem of privacy policy is that none of the frameworks seems to offer a wholly compelling way forward as technologies develop and capture more and different kinds of data put to new and creative uses. But perhaps the answer is not hitching our wagon to one solution, accepting it's problems and leaving the rest behind. Perhaps a hybrid solution is necessary, one that does not require a phenomenal and likely impossibly comprehensive Act of Congress, but a smaller one that builds a privacy platform based on transparency, accountability, and security.

<!-- If the problem is left to regulatory agencies, however, we can construct a privacy regime that blends Nissenbaum's attention to contextual sensitivity with a way of simplifying and making FIPPs more effective, one that enables citizens to become educated sufficiently to live as comfortably online as off, and for private companies—the data collector-service providers—a clear way of knowing whether they are in compliance with the applicable laws. -->

<!-- Make a "Privacy highway": a platform for action -->
Tim O'Rilley (2010) champions a concept called "Government As A Platform" (GAAP) wherein government exits alongside other successful technological platforms. O'Riley's model puts government first as the infrastructural provider for a society based on openness and interoperability. Good policy builds a simple system to tackle big problems. In the book and elsewhere O'Rilley is fond of praising "the Federal-Aid Highway Act of 1956 which committed the United States to building an interstate highway system," as an exemplar of GAAP policymaking. The Highway Act set up a network atop which a whole host of previously difficult innovation could be done. In that case, the government was able to make laws regulating traffic and safety on the highways themselves, "interstate commerce,...gasoline taxes and fees on...vehicles that damage the roads,...speed limits, specifying criteria for the safety of bridges, tunnels, and vehicles that travel on the roads...as the 'platform provider.'" But once on the platform (i.e., driving on the highways), Americans could go anywhere the highway could take them, establish "factories, farms, and businesses" that use the network, and collectively strengthen the innovation society of post-war America.

The key lesson of the Federal-Aid Highway Act, O'Rilley argues, for future policies, is that the government works best when it "invests in infrastructure (and 'rules of the road') that will lead to a more robust private sector ecosystem." The key problem with existing privacy frameworks is that they put government in the role of competitor; government, in these models, would attempt to impress upon private parties, citizens and business entities alike, a constraining regulation. A better solution would be for the government to lay out a platform, or set of platforms, that at once protect citizen data and promote continued proliferation of innovative technologies.

<!-- Start with the FPA -->
This solution begins with pervasive transparency from the start by compelling all federal agencies to create a data handling policy accessible to all citizens. It builds off the existing Federal Privacy Act, first passed in 1974, which governs how the federal government must handle citizen data collected and stored in electronic databases. The FPA laid down a FIPPs-based set of rules for federal agencies:

1. Openness: no database should exist in secret
2. Individual Access: a citizen should be able to access and correct information stored about him in a database
3. Collection: there must be a limit to what you collect, and it should only be done in a certain way
4. Use: data can only be used for the specific purpose described and limited to that use
5. Disclosure: Data are not to leave the database unless authorized and disclosed to the citizens
6. Information management: collection of citizen data must be necessary and lawful
7. Accountable: the agency must stand by the integrity of its database and accept penalties if it is breached

<!-- Modify it, and compel transparency -->
Congress should take these seven principles (or a revised set of them) and instruct every federal agency to elucidate a clear policy for how any information collected from a citizen is stored, used, transferred, and protected under the FPA. Additionally, if the agency requires long-term storage of personally identifiable information (nearly all information these days is PII) it should make clear for how long data will be stored in the database, for what purpose, and what mechanisms citizens have to gain access to data stored about them and correct that information if it is wrong. The government, after all, is no stranger to data collection from citizens. Indeed, citizens are compelled to inform the government of their income, marital status, home address, and a host of other data just to pay taxes. As unmanned aerial vehicle surveillance and law enforcement technologies advance, it will be easier for the government to quietly collect more data about citizens than has ever previously been available.

<!-- Documentation, key two -->
On top of transparent practices comes documentation. It is one thing to declare the process is operating a certain way, but government agencies should be compelled to show how their data storage practices work in as detailed a way as possible without compromising the integrity of the database. This will allow any concerned citizens to have that information, but will also allow others to structure their policies in similar ways.

<!-- Tagging and tracking data -->
Finally, since transparency is of primary concern, another up-front need would be the creation of a standardized method for tagging and tracking data as they flow through a computerized system. Paula Bruening and K. Krasnow Waterman who sketched a system of affixing metadata to bits of information that can be used to follow and govern their flow through a system. <!-- In our IRS example, each tax return might be affixed with a different tag depending how it should be saved for audit purposes. A hypothetical individual's return is processed and audited and then tagged _three-years_, after which it is either deleted or extended for another three. This, again, is just an example, tax law is complicated, but it serves to illustrate the point that information could be tagged and sorted just as easily as it can be collected an analyzed. --> Once tagged, simple algorithms can be written to determine which data can flow to which parts of the greater system. The mechanisms used by the government for tagging and tracking data through its system should be as openly documented and available to private parties as the rest of the privacy policy, and the system used to assign tags, as openly available as the Library of Congress's system for cataloging books.

<!-- Invite industry to join -->
Finally, these policies will be used as models for industry to mirror. Private actors collecting data will be invited to voluntarily adopt the federal privacy policies to become privacy-certified. Should a company like Facebook choose to do abide the policy, they will be publicly indexed among other organizations who have all volunteered to handle data in the same way. From this hypothetical point forward, the FTC will use these privacy platforms as the standards by which audits are conducted. Certification will be taken as a promise to consumers and certified companies would be assumed in compliance and immune from audits or prosecution until a consumer fraud complaint was filed at which point the FTC would have grounds to audit them. Any companies not in compliance will be subject to an FTC audit at any time. 

<!-- Audits and promises to consumers the only enforcement -->
This would be the only enforcement mechanism. Just as anyone may drive on the highway until they break one of the "rules of the road," anyone may collect, aggregate and use data for as long as they want until they fall out of line with the open standard they have agreed to follow.

## How it works: A hypothetical model

<!-- An example in the IRS -->
For an example of how this might work, one federal agency collects a particularly large volume of personal information and has since it first came into being: the Internal Revenue Service. While the tax code is far from the kind of Utopian platform O'Rilley imagines, the IRS is a suitable agency for exploring a privacy platform because of the voluminous amount of data collected about nearly every citizen, business, and non-profit entity in the United States every year in the process of filing taxes. A simple 1024-EZ collects information about how much money an individual made in a year, where it came from, and the citizen's address, date of birth, social security number, employer, banking information. As the tax forms get more complicated, more data is collected including large assets, where investments are held, educational expenses, what charities were donated to, political parties supported, and many more details. 

<!-- Records are used for multiple purposes -->
The IRS keeps these records on file primarily to determine whether the individual has over- or under-paid their taxes for that year, but also to determine the next year's tax rate. The database's existence is not secret, and individuals have an opportunity to correct information stored about them in it, and the records are general kept secure except in case of an audit, at which time the records may be used in criminal cases. Citizens also have an opportunity to obtain information stored about them in the database, though they cannot ask for it's removal for obvious reasons.

<!-- There are still a few questions, IRS is not perfect but good a start -->
Citizens can easily find information about how long the IRS needs to keep this information, three to six years, and why, in case of an audit (up to three years) or if a serious error is found in an audit (up to six). But there is also some possibility that older documents could be requisitioned—the statute of limitations may be extended under special circumstances (IRS Website 2013). Even with all this information citizens still do not know what the IRS keeps after that six year mark. They can request the statute of limitations be extended, but for how long? Under what "special conditions?" How far back may it be extended? What is the process IRS requires for transferring tax data to another agency? To private companies? These are the kinds of questions the IRS would have to address in order to build an Open Privacy Platform for tax-related financial information.

## Government As A Platform can Work in Privacy, If we Only Try

<!-- The last thing we need is 1300 privacy policies. -->
One obvious flaw in this plan is the sheer volume of government agencies that would need to create privacy policies. With more than 1300 government agencies operating under the federal government, Fred Cate gets extra impetus behind his question of "which FIPPs?" The operationalization of this idea is difficult, and policymakers would need to find the delicate balance between too many and too few privacy policies. Perhaps each cabinet level agency writes its own. Within each of the 15 departments the variation among different agencies could create workability problems if limited there. Perhaps alongside the directive to create privacy platforms should be built some kind of process for determining whether an agency can adopt another agency's policy, or whether it is so unique as to need its own. Who within the federal government is permitted to create such policies is a problem that can be solved, and more privacy standards can be create as-needed. This is a challenge for implementation, but not an insurmountable one.

Electronic data privacy policy can be made one of two ways. The first, by constraining behavior and penalizing misbehavior, or by establishing a set of public, open standards the federal government will hold itself to and make available to private organizations to voluntarily comply with. The latter allows much more latitude and flexibility for adjusting the policy to meet future needs, but more importantly, it allows the government to achieve a desirable public policy outcome without prohibiting the market from exploring, developing, and innovating technologies that may be profoundly useful to society. It gives citizens more access to the core of what government does to protect their privacy as well as how the might be treated by private service providers.

# References and Sources Consulted
1. Bruening, Paula J., and K. Krasnow Waterman. "Data Tagging for New Information Governance Models." 2010. In _IEEE Security & Privacy Magazine_ 8.5 pp. 64-68
1. Fred Cate, "Failure of Fair Information Practice Principles", Chapter 13 in Winn, J.K. (Eds), _Consumer Protection in the Age of the Information Economy_, 2006, pp. 343-369
1. Nissenbaum, Helen, _Privacy in Context_, 2010, Stanford University Press.
1. Richard Posner, The Right of Privacy, 12 Georgia Law Review 393 (1978) pp.393 - 404
1. Jonas, Jeff and Harper, Jim, "Open Government: The Privacy Imperative", in Ruma, Laurel and Steele, Julia eds., _Open Government_, 2010, O'Riley Media, Sebastopol, CA
1. Mayer Schönberger, Viktor, _Delete: The Virtue of Forgetting in the Digital Age_. 2010. Paperback. [Publisher, City]
1. O'Riley, Tim, "Government As A Platform", in Ruma Laurel and Steele, Julia eds., _Open Government_, 2010, O'Riley Media, Sebastopol, CA.
1. White House Cabinet. 2013. http://whitehouse.gov/administration/cabinet