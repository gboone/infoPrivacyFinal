<!-- 
A proposal for an Open Policymaking Interface, a new model for privacy policy.
-->
# Protect my Data

## Introduction

<!-- Goog settlement with FTC -->
Search and Internet services giant Google recently settled a privacy dispute with the FTC with a penalty of $7 million and an instruction to beef up their privacy. At issue in that case was not any invasion of privacy carried out by the company's photo-snapping cars, but the actions of a few of their drivers who took advantage of security flaws in household wireless networks to steal passwords, emails, and troves of other personal information from private computers. At the same time Google's biggest competitors have taken aim at the company's handling of personal data through their email, search and other Internet tools. Microsoft's *scroogled* marketing campaign highlights the 'reading of email' Google does to serve ads to email and data sharing practices with third-party advertisers.the big question in scenarios like these is, 'why surrender personal data if you don't have to?'

<!-- SCOTUS -->
From another angle, the Supreme Court of the United States considered a case this year that asked the extent to which there was a legitimate interest in sharing of medical information between doctors and pharmaceutical companies. Should the pharmaceutical companies be allowed to turn those data over to a third party processor for marketing purposes, or do the companies have a First Amendment right to do as they please with the data? The third-party doctrine adopted first in telecommunications privacy policy offers some guidance: that once data is surrendered, the third party may do as he pleases with it. A countervailing interest lies with our understanding of health care privacy under HIPAA, information exchanged between a doctor and patient should not breech that dyad except under special circumstances, and marketing does not seem to be among them. 

<!-- Thesis statement: a privacy policy based on open gov, focused on education and encouragement -->
These two situations are a good transom into some of the more puzzling privacy issues policymakers face with information privacy, these, added atop some other classic examples (people losing jobs to Facebook pictures in the wrong hands, people being charged with crimes based on evidence retrieved from digital archives, and so on), further exacerbate the anxiety policy makers should experience when thinking about the legislative lift required to 'solve the problem.' But what if the problem is simpler than it seems, so simple that an act of Congress may be over-solving the problem. This paper will argue for a privacy policy for the United States based on principles of open government, borrowing lightly from the world of computer programming, and focused on encouraging good behavior among private entities and education among citizens.

## The problem: not breaking anything

Data surround us. Researchers Bounie and Gile estimated the volume of data produced globally in 2008 was nearly 15 exabytes, or 16 billion gigabytes or two billion full DVDs. Some of these data are being collected by private companies who often store them and send them to other third-party processors or sell them to advertisers to build web services. This exchange of personal data is not only what keeps these services alive, but also what makes them work so well. Google's Gmail service, for example, uses data about who you email and how frequently to try and guess who you mean when you start typing just a few letters. It also recommends other people who you frequently email in conjunction with others to include on a message. When I email co-worker X, it recommends co-worker Y and friend Z, because I often email X, Y, and Z together. 

These kind of predictive services are amazingly efficient. Before Google rolled out Instant search in 2010 the typical search took about 9 seconds to type. Since then they estimated "more than 3.5 billion seconds" will be saved each day simply by reducing that nine seconds down to between two and five ([Google, 2010.](http://www.google.com/insidesearch/features/instant/about.html)). This product built off of Google's existing search algorithms, but also the data collected about what people _mean_ when they search. Without the collection of troves of data about what word it's users most frequently completed when they type a given combination of letters, they could not have built such a well functioning tool. 

The drawback, though, is what Google do with those data outside of building that well-functioning tool. They also share some of those data with advertising companies, some of which they own, to generate online ads for their users. Google's business model is built on this kind of sharing. What those advertisers do with Google users's data, who they sell it to, and what those people do with it are completely hidden from view. When a law enforcement agency goes to Google or one of the partners they have sold data to, what happens to those data? Do those agencies need warrants to access it? There are real and complicated questions at stake here.

Google search competitor DuckDuckGo offers a search tool that does not collect person data, and is in some ways an inferior product because of it. A search for "Boston Marathon" on Tuesday, April 16, 2013, a day after the notorious bombing of the world's oldest annual city marathon, on DuckDuckGo showed only generic results about the race, nothing about the tragedy. Google, comparatively, was able to deliver three news results at the top of the results, and an additional eight results in the normal search results. Despite displaying more results and not keeping a copy of any search data, DuckDuckGo's results were inferior. Similarly, searches for Cyber Intelligence Sharing and Protection Act (CISPA) and the Electronic Communications Protection Act (ECPA) returned generic results and old news articles on DuckDuckGo while Google led off with current news as well as generic information about these active bills and debates before the American Congress.

Herein lies the tension between privacy and making things that work. Most of Google's products would not work as well without the data they sell to their advertisers and put to work in their products. They certainly would not have the capital to make new products without selling something, and clickthrough data are easier to sell than other models like subscriptions and premium services. On the one hand, an individual can search DuckDuckGo quietly and need not worry about third parties connecting search terms back to him or her. On the other they are getting a clearly inferior product. Searches for _Boston Marathon_, _Boston Marathon 2013_ both returned nothing related to the bombing, while a search for _Boston Marathon bombing_ did, few of the results were from major media outlets whereas Google's results floated links to the New York Times, CNN, and Fox above blogs and smaller and less reputable sources. In fact, after the 14th result, none of DuckDuckGo's results even mention the bombing. While not all of the discrepency can be linked directly to Google's use of user data to inform searches, the differences are telling and make Google's products invaluable resources.

Other technologies face similar issues. Unmaned aerial vehicles, so-called 'drones,' have the potential to give law enforcement as well as private individuals the ability to use cameras and other sensors mounted on an aircraft to gather data about the world around them. At the same time they could be used by the digital peeping Tom who seeks to spy on his neighbor, or by a national security program to track individuals without their consent. This list goes on and on, from GPS trackers in cellphones and automobiles, to Internet services powering the devices that make up the 'Internet of things,' data collection, storage, aggregation and processing are both technology's linchpin of effectiveness and it's most dangerous component. 

Any legislative action taken on information privacy must be done with special attention given to this delicate balance between building good technology, and violating legitimate privacy interests. First party processing, that is, should be a high priority for lawmakers seeking to solve the information privacy problem. While solutions like Mozilla Firefox's default third-party cookie blocking and the Do Not Track signal standard built into other browsers are compelling, they only solve part of the problem. These solutions give users more control over what is shared, but leave them still without much guidance as to how to discover what happens to the data they do share, and what they can do about it.

Past regulation attempts have fail to respond effectively to the problems thrown by these new technologies. This is in part because they are outdated. ECPA was enacted some 27 years ago, and the only law passed since pertaining to electronic data transmissions applies only to children (the Children's Online Privacy Protection Act, [COPPA]). There is a sense of urgency to correct this: the longer Congress waits to act, the further technology will progress, the larger the databases will grow, and the more out of hand the issues around data use will become. The European Union sought to update its 1995 directive on electronic privacy in 2013 using the one policy to rule them all approach they employed in the past, but the United States has yet to enact any new legislation, mostly leaving any online privacy action to agencies like the Federal Trade Commission (FTC) to apply what laws are already on the books to the online world. US agencies have crafted a plurality of privacy recommendations and policies mostly derived from some understanding of the Fair Information Practices Principles (FIPPs), a practice that lends some credibility to Helen Nissenbaum's ideas about privacy being context specific.

Indeed health care, financial services, the Internet, children on the Internet, and law enforcement all seem like discrete areas where privacy should be protected distinctly from the others. Nissenbaum argued that the right to privacy has different social norms attached to it depending on where it manifests itself. In financial services, for example, there is economic incentive for society to individuals being forced into the credit system (fewer bad loans being issued) and to the individuals themselves (potentially better rates and access to credit when it is needed). In this case our right to privacy is consistent with the expectations we have of other parties.

Nissenbuam's framework runs into problems when the data need to be used in more than one context.